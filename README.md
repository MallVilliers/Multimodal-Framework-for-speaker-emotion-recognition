# Multimodal-Framework-for-speaker-emotion-recognition
IEMOCAP Emotion Recognition with Cross-modal Attention and Speaker Information
This repository implements a novel deep learning architecture for emotion recognition in multimodal data, achieving state-of-the-art performance on the IEMOCAP dataset.

Key Contributions
Cross-modal Attention Mechanism: A novel attention mechanism is designed to effectively combine audio and visual features, capturing the complex relationships between these modalities for improved emotion recognition accuracy.


Advanced LSTM Architecture with Speaker Information: A specialized LSTM architecture takes speaker-specific information into account, allowing the model to make more nuanced emotion predictions based on individual speaking styles.


Learnable Weight Fusion Approach: The model learns optimal weights for combining audio and visual data dynamically, adapting to different data sources and recording conditions.


State-of-the-Art Performance: The model achieves 70.43% accuracy on the 6-category emotion recognition task on the IEMOCAP dataset, outperforming existing methods like Dialogue RNN, Dialogue GCN, EmoBERTa, and COSMIC.
